Information about deep Learning:
Why deep learning?: As the size of data increase, the performace of deep learning increases almost linearly. But usual classical ML argorithms does reach to limit quickly.
Deep Learning help automatically feature engineering (feature extraction): Deep learning handle the feature extraction and Model simulateniously. In ML project, feature exraction
and training the model are two different steps.

Complex problem can solve complicated problication such as image classification, object detection and NLP Problem

Deep Learning: ANN, CNN, RNN
First step-Neural network (first nn is Perceptron)

ANN- Forward Propogation = Z = Act(y); y =a.w + b and activation function ex is sigmoid such as (1/(1+e^-y)
Activation function: Sigmod, tanh, Relu (max(y,0))

Back propogation is the process by which we adjust the parameters in the neural network as w_new = w_old - eta * (dL/dw) (derivative of loss function, cost function)

Vanishing gradient: mainly to sigmoid function we see the vanishing gradient. When we use the signoid function as a activation funcation in the deep neural network some of the
derivative values (in the back propagation) are very small (derivative of sigmoid function is from 0 to 0.25). During the back propagation, we have a chain rule and need to multiple
multiple derivatives terms. when many small values get multiplied, the resultant value becames almost zero as a result new parameter will have the almost value as the old one
there is no change in the parameter and there is no learning. To get rid of this problem, we do not use the sigmod function in the deep NN. tanh activation function also will have
this issue (the range of the derivative of this activation function is 0-1).
Exploding Gradient Problem: When the initial weight values are large, sometime in the back -propagation we get to multiple multiple terms. As a reuslt our derivative term might be
very big in this equation w_new = w_old - eta * (dL/dw). When the second term is very large, our w_new will be very large with negative value and this issue won't let conververg
to the minimum point. this is know as exploding gradient problem.

Threshold activation function- tanh(x), derivate range is again (0,1)

Drop out and Regulation: When the neural network is very large, there are so many parameter to fit and ofen time it overfit the training data set. To fix this problem we get to
use drop out technique. The drop out technique is almost like a random forest approach. In each iteration, different features and nodes will be selected and other will be excluded.
But when it go through several iterations, it will touch upon all the nodes and features.

Rlu- Rectificed Linear Unit: The derivative of RLu function is either 0 or 1 (=0 if z <0 else 1). As a result of this, the derivative term in equation w_new = w_old - eta * (dL/dw) will
either be 1 or 0. If 1, it is all right. But if zero, them the w_new and w_old remain same and basically this particular neuron will be dead. to not make the derivative exactly
zero, we can consider leaky RLu function, which has small derivative value when z is negative. 

Leaky RLu = max(0.01x, x). If there are many negative weight values which causes the derivative value = 0.01 and in back propagation there could be multiple 0.01 must be
multiplied and as a result vanishing gradient still persist. 

sigmoid is not zero centered since the data do not pass through zero so the computation takes little longer, whereas the tanh is zero centered and computational efficiency.
Thus tanh is better than sigmoid but both suffer from vanishing gradient as both derivative rang from 0 to 0.25/1.0.

ELU - Exponential Liner Units: x if x > 0 else a*(exp(x) - 1)

PRelu (Parametric ReLU)

Swish (A self Gated Function): (applied when you have more than 40 layers)  y = x *sigmod(x)

Soft max: multiobject classification (last layer of the nn)
(https://www.dlology.com/blog/how-to-choose-last-layer-activation-and-loss-function/#:~:text=Last%20layer%20use%20%22softmax%22%20activation,of%20our%2010%20digit%20classes)

# Weight Initialization: Key oints: Weights should be small, should not same, should have good variance
Using Uniform distribution (-1/sqrt(fan_in), 1/sqrt(fan_in)): fan_in = number of input feature
Xavier/Gorat: Xavier Normal- w ~ N(0,sig) where sig = sqrt(2/(fan_in + fan_out))
Xavier Uniform 












